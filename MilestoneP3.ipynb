{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow import of custom modules from the \"src\" folder\n",
    "import sys\n",
    "sys.path.insert(0, 'src/')\n",
    "\n",
    "# Standard packages for data analysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Web scraping\n",
    "from scrape_wiki import PageviewsClient\n",
    "from update_keywords import read_keywords\n",
    "\n",
    "# Other\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path management\n",
    "data_path = \"data/\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists('report'):\n",
    "    os.mkdir('report')\n",
    "if not os.path.exists('figs'):\n",
    "    os.mkdir('figs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make incredibly good looking plot with ipypublish, which basically\n",
    "# overrides matplotlib default parameters and make plots more latex-looking\n",
    "\n",
    "# Enable fancy plots if you want to generate plots for the article\n",
    "# otherwise keep False, since it lengthens rendering time\n",
    "fancy_plots = False\n",
    "\n",
    "if fancy_plots:\n",
    "    from ipypublish import nb_setup\n",
    "    rcparams = {\n",
    "        'axes.titlesize':13,\n",
    "        'axes.labelsize':7,\n",
    "        'xtick.labelsize':6,\n",
    "        'ytick.labelsize':6\n",
    "    }\n",
    "    plt = nb_setup.setup_matplotlib(rcparams=rcparams)\n",
    "    sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TODOs** \n",
    "\n",
    "* Modify titles numbers (I, II, III) once it won't change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Keywords selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDPR-related keywords\n",
    "\n",
    "Selected GDPR-related keywords was mainly done by manual selection of relevant articles, we simply wrote them in a text file and we load them in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path and settings\n",
    "keywords_GDPR = data_path + 'GDPR_{}.txt'\n",
    "language = 'de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of keywords with our custom function\n",
    "keywords = read_keywords(keywords_GDPR.format(language))\n",
    "sorted(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular articles\n",
    "\n",
    "One can find the selection of popular Wikipedia articles under the Web Scraping section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Web Scraping\n",
    "\n",
    "In this section, we reproduce the steps performed to obtain the dataset we will work on. This is mainly a summary of what is shown in the [Scraping](Scraping.ipynb) notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "According to the [Wikipedia REST API rules](https://wikimedia.org/api/rest_v1/), we must specify a contact address as the `User-Agent` for queries to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set contact address\n",
    "contact = 'matthias.zeller@epfl.ch'\n",
    "\n",
    "# Instanciate query function\n",
    "p = PageviewsClient(contact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to request article views\n",
    "params = {\n",
    "    'agent': 'user',\n",
    "    'start': '20150401', # 1st April 2015\n",
    "    'end':   '20190531'  # 31th May 2019\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Helper functions\n",
    "def request_top(dates, domain=language, limit=500):\n",
    "    \"\"\"\n",
    "    Wraps the function PageviewsClient.top_articles.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    dates: list \n",
    "           List of dictionnaries with keys 'year', 'month', 'day'\n",
    "    domain: str\n",
    "            Specifies the Wikipedia project (en, de, fr, ...)\n",
    "    limit: int\n",
    "           Number of top articles to fetch for a given date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with columns year, month, day, views, article, rank\n",
    "    \"\"\"\n",
    "    def fetch_date(d):\n",
    "        try:\n",
    "            df = p.top_articles(domain, **d, limit=limit)\n",
    "        except Exception:\n",
    "            # Happens if data not available\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(df)\n",
    "        df['year'] = d['year']\n",
    "        df['month'] = d['month']\n",
    "        df['day'] = d['day']\n",
    "        return df\n",
    "    \n",
    "    domain = domain + '.wikipedia'\n",
    "    \n",
    "    # Fetch with parallel requests\n",
    "    with ThreadPoolExecutor(10) as executor:\n",
    "        res = list(executor.map(fetch_date, dates))\n",
    "    \n",
    "    # Format results in a DataFrame\n",
    "    res = pd.concat(res, ignore_index=True)\n",
    "    # Replace None -> np.nan\n",
    "    res = res.applymap(lambda elem: np.nan if elem is None else elem)\n",
    "    \n",
    "    res['language'] = domain.split('.')[0]\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def request(articles, domain=language, **kwargs):\n",
    "    \"\"\"\n",
    "    Wraps the function PageviewsClient.article_views\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    articles : list\n",
    "              List of Wikipedia article names\n",
    "    domain : str\n",
    "            Wikipedia project (de, en, es, fr...)\n",
    "    kwargs\n",
    "            Additionnal arguments that override `params` and passed to article_views()\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "         Dataset with columns being articles and index are dates\n",
    "    \"\"\"\n",
    "    wrapped_kwargs = params.copy()\n",
    "    wrapped_kwargs.update(kwargs)\n",
    "    domain = domain + '.wikipedia'\n",
    "    \n",
    "    # Fetch\n",
    "    res = p.article_views(articles=articles, project=domain, **wrapped_kwargs)\n",
    "    \n",
    "    # Format results in a DataFrame\n",
    "    res = pd.DataFrame(res).T\n",
    "    # Replace None -> np.nan\n",
    "    res = res.applymap(lambda elem: np.nan if elem is None else elem)\n",
    "    # Sort by dates\n",
    "    res.sort_index(inplace=True)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular articles selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use (a modified version of) the `top_articles` function from the [python-mwviews](https://github.com/mediawiki-utilities/python-mwviews/blob/master/mwviews/api/pageviews.py) package to fetch the most popular Wikipedia articles for our given time period. We fetch articles that are popular during a whole month with `day='all-days'` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following strategy:\n",
    "\n",
    "1. Fetch the monthly top 400 articles for every date specified\n",
    "1. Compute and retain only the articles that appear as a top article in each month\n",
    "1. Filter out special articles (e.g. home page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time range of interest\n",
    "dates = [\n",
    "    {'year': year, 'month': month, 'day': 'all-days'}\n",
    "    for year in [2015, 2016, 2017, 2018]\n",
    "    for month in range(1, 13)\n",
    "] + [\n",
    "    {'year': 2019, 'month': month, 'day': 'all-days'}\n",
    "    for month in range(1, 6)\n",
    "]\n",
    "\n",
    "top = request_top(dates, limit=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check\n",
    "top.groupby(['year', 'month']).article.nunique().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indeed have 400 articles per entry (i.e. per month). We now retain the articles that appear in each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize iterator for sets of articles per month\n",
    "it = (set(article_lst) for _, article_lst in top.groupby(['year', 'month']).article)\n",
    "\n",
    "# Compute intersection of lists of articles\n",
    "intersect = list(reduce(\n",
    "    lambda accumulator, article_list: accumulator.intersection(article_list),\n",
    "    it\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of \"special\" articles\n",
    "specials = [\n",
    "    'Spezial:', 'Wikipedia:', 'Datei:', 'Benutzer:', 'Special:', 'Hauptseite'\n",
    "]\n",
    "\n",
    "# Discard special articles from the current list\n",
    "selection = [\n",
    "    art for art in intersect if not any(map(lambda special: special in art, specials))\n",
    "]\n",
    "\n",
    "print(f'Full list has length {len(intersect)}, filtered list has length {len(selection)}')\n",
    "selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract those 39 popular articles and format a data set suited for downstream analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract selected articles\n",
    "ctrl = top[top.article.isin(selection)].reset_index(drop=True).copy()\n",
    "# Create date column\n",
    "ctrl['date'] = list(map(lambda tpl: f'{tpl[0]}-{tpl[1]}', zip(ctrl.year, ctrl.month)))\n",
    "ctrl.date = pd.to_datetime(ctrl.date)\n",
    "# Keep relevant columns\n",
    "ctrl = ctrl[['article', 'date', 'views', 'language']]\n",
    "ctrl.columns = ['Article', 'Date', 'Pageviews', 'Language']\n",
    "\n",
    "ctrl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def format_dataset(df, language):\n",
    "    out = pd.DataFrame(df.unstack()).reset_index()\n",
    "    out.columns = ['Article', 'Date', 'Pageviews']\n",
    "    out['Language'] = language\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the helper function \"request\" defined above\n",
    "# and the list of keywords also loaded above\n",
    "df = request(keywords, language)\n",
    "df = format_dataset(df, language)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "The Wikipedia REST API cannot provide data anterior to July 2015, so we either have missing values (as in `df`) or do not take top articles from April 2015 to June 2015 into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# I. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Cleaning and handling missing/undefined values\n",
    "\n",
    "**TODO** remove ? *Undefined values are any non-positive numerical or non-numerical values. In case of missing values, we will discuss the possible fixes and consequently decide on our way to handle them.*\n",
    "\n",
    "We check missing values in the study group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_missing = df.isna().any(axis=1)\n",
    "df[mask_missing].sort_values('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we don't have any information before 1st July 2015, since it is not available with the API. We drop those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~mask_missing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for control group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl[ctrl.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing has to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Consistency in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot and examine each article views individually to identify outliers and absurd data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def plot_pageviews_grid(df_longitudinal, n_col=5, yscale='log', rolling_window=None):\n",
    "    \"\"\"Plot the time series of several articles, each having its individual subplot (\"small multiples\").\n",
    "        \n",
    "    Parameters \n",
    "    ----------\n",
    "    df_longitudinal : pandas.DataFrame\n",
    "        pageviews in longitudinal format\n",
    "    n_col : int\n",
    "        number of columns for the plot, number of rows are automatically deduced\n",
    "    yscale : string\n",
    "        log or linear, passed to ax.set_yscale\n",
    "    rolling_window : int\n",
    "        size of the mean rolling window, units depends on dates in df_longitudinal\n",
    "    \"\"\"\n",
    "    if df_longitudinal.Language.nunique() > 1:\n",
    "        raise ValueError\n",
    "        \n",
    "    articles = df_longitudinal.Article.unique()\n",
    "    if 'Date' in df_longitudinal.columns:\n",
    "        df_longitudinal = df_longitudinal.set_index('Date')\n",
    "    if yscale == 'log':\n",
    "        df_longitudinal = df_longitudinal.replace(0, np.nan)\n",
    "    \n",
    "    # Init plotting\n",
    "    colors = sns.color_palette(n_colors=len(articles))\n",
    "    n_row = math.ceil(len(articles) / n_col)\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3*n_col + .5, 2.25*n_row + .5), sharex=True, sharey=True)\n",
    "    \n",
    "    # Plotting loop\n",
    "    for article, ax, col in zip(articles, axes.ravel(), colors):\n",
    "        # Extract article data\n",
    "        subdf = df_longitudinal[df_longitudinal.Article == article].copy()\n",
    "        # Smooth data\n",
    "        if rolling_window is not None:\n",
    "            subdf.Pageviews = subdf.Pageviews.rolling(rolling_window).mean()\n",
    "        # Plot\n",
    "        subdf.plot(y='Pageviews', ax=ax, legend=False, color=col)\n",
    "        article = article.replace('_', ' ')\n",
    "        ax.set_title(article)\n",
    "        ax.set_yscale(yscale)\n",
    "        \n",
    "    # Aesthetics\n",
    "    for ax in axes[-1]:\n",
    "        ax.set_xlabel('Date')\n",
    "    for ax in axes[:, 0]:\n",
    "        ax.set_ylabel('Views')\n",
    "        #ax.tick_params('y', left=True, which='minor',)# reset=True)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Make this a raw cell (click on left part of cell and press R) or code cell (press Y)\n",
    "# Plots take time to render\n",
    "plot_pageviews_grid(df, rolling_window=3)\n",
    "plt.savefig('figs/pageviews_grid_study.eps')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_pageviews_grid(ctrl)\n",
    "plt.savefig('figs/pageviews_grid_control.eps')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers articles removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles outliers in each of the groups depending on the results obtained in the small multiples above.\n",
    "\n",
    "For the treatment group:\n",
    "- we remove the following articles: Individualrecht, Datenvernichtung, Zensur im Internet, Vorratsdatenspeicherung. Indeed, these artciles have a unique spike at different moments during the period of analysis, which causes outliers in our aggregation of all articles.\n",
    "- we remove Cyber-terrorismus article. This article is sensitive to the cyber attacks news and therefore oscillate a lot. This will hence impact our total articles pageviews when it isn't a critical keyword for the GDPR analysis. Therefore it was removed.\n",
    "\n",
    "For control:\n",
    "- we remove the articles which have a unique important spike. This is because we assume there was an important event which has impacted the trend and thus resulted in the unique spike. These articles are the following: Italien, Vereinigtes Königreich, Berlin, Hamburg, Polen, Niederlande.\n",
    "- we remove the movies articles because their trend depend on the announcement of a new season. There articles are the following: Game of thrones, Grey's Anatomy et The Walking Dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of articles outliers to remove\n",
    "outliers_treatment = ['Individualrecht', 'Datenvernichtung', 'Zensur_im_Internet', 'Vorratsdatenspeicherung', \\\n",
    "                     'Cyber-Terrorismus']\n",
    "outliers_control = ['Italien', 'Vereinigtes_Königreich', 'Berlin', 'Hamburg', 'Polen', 'Niederlande', \\\n",
    "                   'Game_of_Thrones', 'Grey’s_Anatomy', 'The_Walking_Dead_(Fernsehserie)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outliers from the treatment and control groups\n",
    "df = df[~df['Article'].isin(outliers_treatment)]\n",
    "ctrl = ctrl[~ctrl['Article'].isin(outliers_control)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Monthly Aggregation\n",
    "The data in our reference paper $\\text{Chilling Effects: Online Surveillance and Wikipedia Use}$ (*Jonathan W. Penney*), data is aggregated on a monthly basis and further computations are done on this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_monthly(df) :\n",
    "    \"\"\"\n",
    "    Aggregates the pageviews in a monthly fashion. Returns a DataFrame with only the numerical columns summed.\n",
    "    Note : Sets the index to date.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : pd DataFrame\n",
    "        df should have at least 3 columns containing `Article`, `Date` and `Pageviews`. Column with `Date` should be named 'Date'.\n",
    "    \"\"\"\n",
    "    # Aggregate monthly\n",
    "    df_month = df.set_index('Date')\n",
    "    df_month = df_month.resample('M').sum()\n",
    "    # Verify period\n",
    "    print(f\"The formatted dataset correctly spans from {df_month.index.min().date()} to {df_month.index.max().date()}.\\n\")\n",
    "    \n",
    "    return df_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of views of treatment and control wikipedia article per month.\n",
    "df_monthly = agg_monthly(df)\n",
    "df_monthly_ctrl = agg_monthly(ctrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers monthly removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the January monthly agrgegated pageviews as they have an important drop of pageviews in many articles in the treatment and control groups, as illustrated on the small multiples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the January aggregated pageviews\n",
    "df_monthly = df_monthly[~(df_monthly.index.month == 1)]\n",
    "df_monthly_ctrl = df_monthly_ctrl[~(df_monthly_ctrl.index.month == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_ctrl.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# II. Generic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Mean Computation\n",
    "The first analysis technique done in the paper is a comparison of means before and after the interruption event. Let us write the methods for this purpose given a monthly pageviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_around_interruption(df, interruption, verbose=False) :\n",
    "    \"\"\"\n",
    "    Divides the dataset into a pre-interruption dataset and a post-interruption dataset. Returns 2 dataframes in (pre, post) order.\n",
    "    Note : The month of the interruption is included in the post-interruption dataset.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : pd DataFrame\n",
    "        Pandas DataFrame containing the monthly pageviews around the interruption date.\n",
    "    interruption : string\n",
    "        Interruption date in the form of a string. Please use following fashion :\n",
    "        1st of May 1997 --> 1995-05-01\n",
    "    verbose : bool (default: False)\n",
    "        Display temp dataframes created in the process and other insightful information from the process.\n",
    "    \"\"\"\n",
    "    # Get pd.Timestamp object\n",
    "    revelations = pd.to_datetime(interruption)\n",
    "\n",
    "    # Creating both datasets\n",
    "    pre = df.loc[df.index < revelations]\n",
    "    post = df.loc[df.index > revelations]\n",
    "    if verbose :\n",
    "        print(\"Pre-interruption pageviews:\\n\", pre)\n",
    "        print(\"\\nPost-interruption pageviews:\\n\", post)\n",
    "\n",
    "    # Verifying operation\n",
    "    if verbose : \n",
    "        print(\"Verifying coherence for split around interruption date...\")\n",
    "    assert(df.size == (pre.size + post.size))\n",
    "    if verbose :\n",
    "        print(df.size == (pre.size + post.size))\n",
    "        \n",
    "    return pre, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_around_interruption(df, interruption, verbose=False) :\n",
    "    \"\"\"\n",
    "    Computes the mean number of pageviews before and after the interruption date. \n",
    "    Returns 2 means (2-tuple of numpy.float64) in (pre, post) order.\n",
    "    Note : The month of the interruption is included in the post-interruption dataset.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : pd DataFrame\n",
    "        Pandas DataFrame containing the monthly pageviews around the interruption date.\n",
    "    interruption : string\n",
    "        Interruption date in the form of a string. Please use following fashion :\n",
    "        1st of May 1997 --> 1995-05-01\n",
    "    verbose : bool (default: False)\n",
    "        Display temp dataframes created in the process and other insightful information from the process.\n",
    "    \"\"\"\n",
    "    pre, post = divide_around_interruption(df, interruption, verbose=verbose)\n",
    "    return pre.mean()[0], post.mean()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. ITS Regression \n",
    "Following the formula of segmented regression of an interrupted time series (ITS) : \n",
    "$$\n",
    "Y_{t} = \\beta_{0} \\cdot \\text{time}+\\beta_{2} \\cdot \\text{intervention}+\\beta_{3} \\cdot \\text{postslope}+\\epsilon_{1}\n",
    "$$\n",
    "We define in the following the methods to perform such a regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_around_date(df, ref_date, time_delta):\n",
    "    \"\"\"Extract data from `df` corresponding to dates ref_date +/- time_delta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe containing at least a 'Date' column (or an index with Dates)\n",
    "    ref_date: string or pandas.Timestamp\n",
    "        Reference date around which to select the data\n",
    "    time_delta: pandas.Timedelta or numpy.timedelta64\n",
    "        Time period to extract from `df` before and after `ref_date`\n",
    "    \"\"\"\n",
    "    ref_date = pd.to_datetime(ref_date)\n",
    "    if 'Date' in df.columns:\n",
    "        df = df.set_index('Date')\n",
    "    output_df = df.loc[(df.index > (ref_date - time_delta)) \\\n",
    "                     & (df.index < (ref_date + time_delta))]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(formula, data, predictor, seed, return_res=False):\n",
    "    '''\n",
    "    Output a linear prediciton using an ordinary least squares linear regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    formula : string\n",
    "        the equation describing the model using patsy formula syntax\n",
    "    data : pd.DataFrame\n",
    "        the dataframe containing the monthly grouped time series elements for fitting\n",
    "    predictors : pd.DataFrame\n",
    "        the dataframe containing the monthly grouped time series elements for predicting\n",
    "    seed : int\n",
    "        a seed for consistency\n",
    "    '''\n",
    "    # Declares model\n",
    "    mod = smf.ols(formula=formula, data=data)\n",
    "\n",
    "    # Fits the model (finds the optimal coefficients, adding a random seed for consistency)\n",
    "    np.random.seed(seed)\n",
    "    res = mod.fit()\n",
    "\n",
    "    # Optimal Coefficients\n",
    "    coefficients = res.params.values\n",
    "\n",
    "    # Compute predictions\n",
    "    ypred = res.predict(predictor)\n",
    "    \n",
    "    if return_res:\n",
    "        return ypred, res\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_views_formatting(df, ref_date):\n",
    "    '''\n",
    "    Computes the dataframe containing the monthly grouped time series elements\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        The dataframe containing monthly views, index should be of type datetime64\n",
    "    ref_date: string or pandas.Timestamp\n",
    "        Reference date around which to select the data\n",
    "    '''\n",
    "    # Create indexing for visualizing correctly on x-axis of the plot\n",
    "    df = df.copy().reset_index()\n",
    "    df['time'] = range(1, df.shape[0]+1)\n",
    "    \n",
    "    # Extract the interruption month's index\n",
    "    reference_date = pd.to_datetime(ref_date)\n",
    "    \n",
    "    # Part the indices and the pre and post linear regression by adding an new indexing variable and \n",
    "    # a boolean variable respectively\n",
    "    reference_point = df[(df['Date'].dt.month == reference_date.month) & (df['Date'].dt.year == reference_date.year)].index[0]\n",
    "    df['postslope'] = np.where(df['time'] <= reference_point, 0, df['time'] - reference_point)\n",
    "    df['intervention'] = [0 if x <= reference_point else 1 for x in df.time] \n",
    "    \n",
    "    return df, reference_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_regression_summary(res, file_name):\n",
    "    \"\"\"Save the latex-formatted regression summary table in a text file (in 'report/' folder).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    res: statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "        The output of smf.ols(...).fit(...)\n",
    "    file_name: string\n",
    "    \"\"\"\n",
    "    latex_str = res_treatment.summary().tables[1].as_latex_tabular()\n",
    "    with open(os.path.join('report', file_name), 'w') as f:\n",
    "        f.write(latex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Plotting\n",
    "The reference paper uses 3 types of plots to describe the results :\n",
    "* **Histogram** of means before and after the interruption event (*cf. figure 1 in reference paper*).\n",
    "* **Segmented regression** around the interrupted event for **1 set** of articles (*cf. figure 2 in reference paper*).\n",
    "* **Segmented regression** around the interrupted event for **2 sets** of articles (*cf. figure 4A in reference paper*).\n",
    "\n",
    "Additionally, the regression plots can feature confidence intervals if prompted by the user (cf. figure 4 in reference paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_mean(df, interruption, ax=None, legend_loc='best', return_df=False) :\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : pd DataFrame\n",
    "        Pandas DataFrame containing the monthly pageviews around the interruption date.\n",
    "    interruption : string\n",
    "        Interruption date in YYYY-MM-DD format. Example: 1st of May 1997 --> 1995-05-01\n",
    "    ax: matplotlib Axes, optional\n",
    "        Axes to draw on.\n",
    "    legend_loc: string\n",
    "        Location of legend.\n",
    "    return_df: bool, optional\n",
    "        Whether to return the pandas DataFrame provided to seaborn.barplot, default is False\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Preparing labels and data\n",
    "    date_str = pd.to_datetime(interruption).month_name()+', '+str(pd.to_datetime(interruption).year)\n",
    "    \n",
    "    \n",
    "    # Pre-post\n",
    "    df['timing'] = f'Pre-{date_str}'\n",
    "    interruption = pd.to_datetime(interruption)\n",
    "    df.loc[df.index > interruption, 'timing'] = f'Post-{date_str}'\n",
    "    \n",
    "    # Hue: time period of 12 months vs 6 months\n",
    "    delta_6m = np.timedelta64(6, 'M')\n",
    "    df['Time span'] = '12 months'\n",
    "    \n",
    "    # To use hue on time span (6 vs 12 months), we must duplicate the data\n",
    "    # for 12 months, filter to keep 6 months around interruption and change 'Time span'\n",
    "    # column values accordingly\n",
    "    df_6m = extract_around_date(df, interruption, delta_6m).copy()\n",
    "    df_6m['Time span'] = '6 months'\n",
    "    \n",
    "    df = pd.concat((df, df_6m))\n",
    "    \n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5,6))\n",
    "        \n",
    "    sns.barplot(y='Pageviews', x='timing', hue='Time span', data=df, ax=ax)\n",
    "    ax.set_ylabel('Mean monthly views', labelpad=15)\n",
    "    ax.set_xlabel('')\n",
    "    ax.legend(loc=legend_loc, framealpha=1.0, title='Time span')\n",
    "    \n",
    "    if return_df:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def segmented_reg_1_set(df, interruption, reference_point, verbose=False, ytick_step=5e5, narts=None) :\n",
    "    \"\"\"\n",
    "    Plots the ITS of a dataset of monthly Pageviews around an interruption date.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : pd DataFrame\n",
    "        Pandas DataFrame containing the monthly pageviews around the interruption date.\n",
    "    interruption : string\n",
    "        Interruption date in the form of a string. Please use following fashion :\n",
    "        1st of May 1997 --> 1995-05-01\n",
    "    verbose : bool (default: False)\n",
    "        Display temp dataframes created in the process and other insightful information from the process.\n",
    "    ytick_step : int (default: 5e5)\n",
    "        Step in pageviews between 2 ticks on the y axis\n",
    "    narts : int (default: None)\n",
    "        Number of articles summed in the monthly Pageviews. Used for the legend.\n",
    "    \"\"\"\n",
    "    # Divide around interruption\n",
    "    pre, post = divide_around_interruption(df, interruption, verbose=verbose)\n",
    "        \n",
    "    ## Plotting\n",
    "    \n",
    "    # Create dataframe with both periods identified\n",
    "    concatenated = pd.concat([\n",
    "        pre.reset_index(drop=True).assign(period='pre'), \n",
    "        post.reset_index(drop=True).assign(period='post')], ignore_index=True)\n",
    "\n",
    "    # Creating a month-identifier column\n",
    "    concatenated = concatenated.reset_index()\n",
    "    concatenated.columns.values[0]='Months'\n",
    "    concatenated['Months'] = concatenated['Months']+1\n",
    "    # The paper shifts monthly count to start from 1.\n",
    "\n",
    "    # Identifying both periods visually\n",
    "    pal = dict(pre=\"black\", post=\"grey\")\n",
    "    g = sns.FacetGrid(concatenated, hue='period', palette=pal, height=5, aspect=1.5, xlim=(-1, 33))\n",
    "\n",
    "    #### TODO use computed regression !\n",
    "    # Creating the regressions for both periods\n",
    "    g.map(sns.regplot, \"Months\", \"Pageviews\", ci=None, robust=1, scatter=False)\n",
    "\n",
    "    # Adding the scatter data separately\n",
    "    g.map(sns.scatterplot,  \"Months\", \"Pageviews\", color=\"black\")\n",
    "\n",
    "    # Configuring the axes to fit the data\n",
    "    plt.xticks(np.arange(0, concatenated.shape[0]+2, 2), np.arange(0, concatenated.shape[0]+2, 2, dtype=int))\n",
    "    ymin, ymax = np.floor(concatenated['Pageviews'].min()/ytick_step)*ytick_step, np.floor(concatenated['Pageviews'].max()/ytick_step +1)*ytick_step\n",
    "    plt.yticks(np.arange(ymin, ymax, ytick_step), np.arange(ymin, ymax, ytick_step, dtype=int))\n",
    "\n",
    "    # Configuring the labels to be as on paper\n",
    "    plt.ylabel(\"Total Views\"+ ( '('+narts+' Wikipedia Articles)' if narts else ''))\n",
    "    plt.xlabel(\"Time (Months)\")\n",
    "\n",
    "    # Adding grid to be as on paper\n",
    "    plt.grid(True, which='major', axis='y', alpha=0.3)\n",
    "\n",
    "    # Creating legend\n",
    "    rev_date= pd.to_datetime(interruption).month_name()+', '+str(pd.to_datetime(interruption).year)\n",
    "    legend = plt.legend(labels=[\"Trend Pre-\"+rev_date, \"Trend Post-\"+rev_date, \"Total Article Views (per month)\"], \n",
    "                        bbox_to_anchor=[1,-0.1], fontsize='medium', \n",
    "                        ncol=2, borderpad=0.75, handlelength=5)\n",
    "\n",
    "    # Adding interruption event line identifier\n",
    "    text_y_coord = plt.gca().get_ylim()[1]\n",
    "    plt.text(reference_point+0.5, text_y_coord*1.05,'Mid '+rev_date,horizontalalignment='center')\n",
    "    plt.axvline(reference_point+0.5, linewidth=2.5, color='black')\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_CI(data, nbr_draws, fit_formula):\n",
    "    '''\n",
    "    A bootstrapping function for obtaining a 95% confidence intervals [lower error, upper error] \n",
    "    around the monthly estimated average.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        the dataframe containing the time series elements\n",
    "    nbr_draws : int\n",
    "        the number of random samples\n",
    "    '''\n",
    "    means = np.empty((0,len(data)))\n",
    "    group_means= np.empty((0, 2))\n",
    "    for n in range(nbr_draws):\n",
    "        indices = np.random.randint(0, len(data), len(data))\n",
    "        data_tmp = data.iloc[indices]#.sort_values(by='index')\n",
    "        ypredi = fit_model(fit_formula, data=data_tmp, predictor=data, seed=n)\n",
    "        means = np.concatenate((means, np.array([ypredi])), axis = 0)\n",
    "    for i in range(len(data)):\n",
    "        group_means = np.append(group_means, np.array([[np.nanpercentile(means[:,i], 2.5), np.nanpercentile(means[:,i], 97.5)]]), axis=0)\n",
    "    return group_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trends_confidence_intervals(df, reference_point, ypred, y_error, color):\n",
    "    '''\n",
    "    Plot the monthly time series and regressions with the confidence interval for the \n",
    "    given dataset and its interruption moment.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : pd.DataFrame \n",
    "        the dataframe containing the time series elements\n",
    "    reference_point : int \n",
    "        the interruption month's index\n",
    "    ypred :pandas.Series\n",
    "        the trend values following the ITS regression model\n",
    "    y_error :numpy.ndarray\n",
    "        the lower and upper 95% confidence interval\n",
    "    color : string\n",
    "        the color for plotting\n",
    "    '''\n",
    "    plt.plot(df['time'][:reference_point], ypred[:reference_point], color=color, linestyle='-')\n",
    "    plt.plot(df['time'][reference_point:], ypred[reference_point:], color=color, linestyle='--')\n",
    "    plt.fill_between(df['time'][reference_point:], y_error[:,0][reference_point:], y_error[:,1][reference_point:], color=color, alpha=0.1)\n",
    "    plt.fill_between(df['time'][:reference_point], y_error[:,0][:reference_point], y_error[:,1][:reference_point], color=color, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(intervention, df_name, reference_point, df, df_pred, df_error, color):\n",
    "    '''\n",
    "    Plot the monthly time series and regressions with the confidence interval for the \n",
    "    given dataset and its interruption moment with labels\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    intervention : string \n",
    "        name of intervention event\n",
    "    df_name : string\n",
    "        name of the group being plotted\n",
    "    reference_point : int\n",
    "        the intervention month's index\n",
    "    treatment_df : pd.DataFrame\n",
    "        the group dataframe containing the time series elements\n",
    "    treatment_pred : pandas.Series\n",
    "        the predicted trend values following the ITS regression model of the group\n",
    "    treatment_error : numpy.ndarray\n",
    "        the lower and upper 95% confidence interval of the group\n",
    "    color : string\n",
    "        the color of the plotting\n",
    "    '''\n",
    "    \n",
    "    # Initialize plot style\n",
    "    sns.set_style(\"darkgrid\")\n",
    "\n",
    "    # Plot trends\n",
    "    plot_trends_confidence_intervals(df, reference_point, df_pred, df_error, color)\n",
    "    \n",
    "    # Add Labels\n",
    "    plt.xlabel(f\"Time [Months]\")\n",
    "    plt.ylabel(f\"Total Views (All Articles)\")\n",
    "    plt.title(intervention);\n",
    "\n",
    "    # Plot scatter points \n",
    "    sns.scatterplot(data=df, x=\"time\", y=\"Pageviews\", color=color)\n",
    "\n",
    "    # Add Legend\n",
    "    plt.legend(bbox_to_anchor=(-0.1, -0.55, 1.2, .102), loc='lower center',\n",
    "               ncol=2, mode=\"expand\", borderaxespad=0., \n",
    "               labels = [df_name + ' Article Trend Pre-' + intervention, df_name + ' Article Trend Post-' + intervention,\n",
    "                         'Confidence Interval Pre-' + intervention, 'Confidence Interval Post-' + intervention, \n",
    "                         'Confidence Interval Pre-' + intervention, 'Confidence Interval Post-' + intervention,\n",
    "                         df_name + ' Articles Views (By Month)'])\n",
    "\n",
    "    # Mark mid-June 2013\n",
    "    plt.axvline(reference_point+0.5, 0, color='black', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# III. GDPR Adoption\n",
    "The European Parliament adopted the GDPR Regulation text on the $27^{th}$ of April $2016$. Our first case of interest are the following research questions at this date : \n",
    "* Is there an inverse chilling effect (e.g. immediate spike) following the announcement of the regulation adoption?\n",
    "* Has there been a change in trends in the viewing of critical terms related to the regulation after the event?\n",
    "\n",
    "We will thus answer these questions using the analysis methods from the reference paper and draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date of interruption event'2016-04-27'\n",
    "interruption_date = '2016-04-27'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep +/- 12 months around interruption date\n",
    "time_delta = np.timedelta64(12, 'M')\n",
    "\n",
    "g_treatment = extract_around_date(df_monthly, interruption_date, time_delta)\n",
    "g_control = extract_around_date(df_monthly_ctrl, interruption_date, time_delta)\n",
    "\n",
    "g_treatment.iloc[[0,1,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_control.iloc[[0, 1, -2, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trends predictions for treatment datasets\n",
    "fit_formula = 'Pageviews ~ time + intervention + postslope'\n",
    "\n",
    "temp_treatment, reference_point = monthly_views_formatting(g_treatment, interruption_date)\n",
    "ypred_treatment, res_treatment = fit_model(formula=fit_formula, \n",
    "                                           data=temp_treatment, \n",
    "                                           predictor=temp_treatment, \n",
    "                                           seed=2,\n",
    "                                           return_res=True)\n",
    "\n",
    "# Compute trends predictions for control datasets\n",
    "temp_control, reference_point = monthly_views_formatting(g_control, interruption_date)\n",
    "ypred_control, res_control = fit_model(formula=fit_formula, \n",
    "                                       data=temp_control, \n",
    "                                       predictor=temp_control, \n",
    "                                       seed=2,\n",
    "                                       return_res=True)\n",
    "\n",
    "ypred_treatment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_regression_summary(res_treatment, 'range_1_treatment.tex')\n",
    "save_regression_summary(res_control, 'range_1_control.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Mean Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "barplot_mean(g_treatment, interruption_date, ax=ax[0])\n",
    "barplot_mean(g_control, interruption_date, ax=ax[1])\n",
    "\n",
    "ax[1].set_ylabel('')\n",
    "ax[0].set_title('GDPR articles')\n",
    "ax[1].set_title('Popular articles')\n",
    "ax[0].legend().remove()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/means_adoption.eps')\n",
    "\n",
    "# ATTEMPT TO MAKE ONE LEGEND ABOVE SUBPLOTS\n",
    "#handles, labels = ax[1].get_legend_handles_labels()\n",
    "#ax[1].legend().remove()\n",
    "#ax[0].legend().remove()\n",
    "#fig.legend(handles, labels, title='Time span', ncol=2, loc=(.5, .8))\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. ITS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove, just to illustrate what's going on\n",
    "g_treatment.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_reg_1_set(g_treatment, interruption_date, reference_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 95% confidence intervals for each monthly estimated average\n",
    "y_error_treatment = bootstrap_CI(temp_treatment, 200, fit_formula)\n",
    "y_error_control = bootstrap_CI(temp_control, 200, fit_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting('April', 'Treatment', reference_point, temp_treatment, ypred_treatment, y_error_treatment, 'tab:orange')\n",
    "\n",
    "plt.savefig('test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting('April', 'Control', reference_point, temp_control, ypred_control, y_error_control, 'tab:blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# GPDR Beggining of enforcement\n",
    "The GDPR regulation was applied starting from the $25^{th}$ of May $2018$ for every country of the European Union (EU) and European Economic Area (EEA). We aim to answer the following questions regarding the event : \n",
    "* Is there an inverse chilling effect (e.g. immediate spike) following the day when the regulation became effective?\n",
    "* Has there been a change in trends in the viewing of critical terms related to the regulation after the event?\n",
    "\n",
    "We will thus answer these questions using the analysis methods from the reference paper and draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Monthly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date of interruption event\n",
    "interruption_date = '2018-05-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep +/- 12 months around interruption date\n",
    "time_delta = np.timedelta64(12, 'M')\n",
    "\n",
    "g_treatment = extract_around_date(df_monthly, interruption_date, time_delta)\n",
    "g_control = extract_around_date(df_monthly_ctrl, interruption_date, time_delta)\n",
    "\n",
    "g_treatment.iloc[[0,1,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_control.iloc[[0, 1, -2, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trends predictions for treatment datasets\n",
    "fit_formula = 'Pageviews ~ time + intervention + postslope'\n",
    "\n",
    "temp_treatment, reference_point = monthly_views_formatting(g_treatment, interruption_date)\n",
    "#temp_treatment.drop(temp_treatment.index[:3], inplace=True)\n",
    "ypred_treatment, res_treatment = fit_model(formula=fit_formula, \n",
    "                                           data=temp_treatment, \n",
    "                                           predictor=temp_treatment, \n",
    "                                           seed=2,\n",
    "                                           return_res=True)\n",
    "\n",
    "# Compute trends predictions for control datasets\n",
    "temp_control, reference_point = monthly_views_formatting(g_control, interruption_date)\n",
    "ypred_control, res_control = fit_model(formula=fit_formula, \n",
    "                                       data=temp_control, \n",
    "                                       predictor=temp_control, \n",
    "                                       seed=2,\n",
    "                                       return_res=True)\n",
    "\n",
    "ypred_treatment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_regression_summary(res_treatment, 'range_2_treatment.tex')\n",
    "save_regression_summary(res_control, 'range_2_control.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Mean Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "barplot_mean(g_treatment, interruption_date, ax=ax[0])\n",
    "barplot_mean(g_control, interruption_date, ax=ax[1])\n",
    "\n",
    "ax[1].set_ylabel('')\n",
    "ax[0].set_title('GDPR articles')\n",
    "ax[1].set_title('Popular articles')\n",
    "ax[0].legend().remove()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/means_enforcement.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. ITS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REMOVE\n",
    "g_treatment.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_reg_1_set(g_treatment, interruption_date, reference_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 95% confidence intervals for each monthly estimated average\n",
    "y_error_treatment = bootstrap_CI(temp_treatment, 200, fit_formula)\n",
    "y_error_control = bootstrap_CI(temp_control, 200, fit_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting('May', 'Treatment', reference_point, temp_treatment, ypred_treatment, y_error_treatment, 'tab:orange')\n",
    "\n",
    "plt.savefig('test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting('April', 'Control', reference_point, temp_control, ypred_control, y_error_control, 'tab:blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
